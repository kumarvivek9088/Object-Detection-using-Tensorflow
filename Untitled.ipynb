{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2745ee0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow_hub as hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "60de074d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    }
   ],
   "source": [
    "model = hub.load(\"https://tfhub.dev/google/openimages_v4/ssd/mobilenet_v2/1\").signatures[\"default\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "faf99e6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    }
   ],
   "source": [
    "model2 = hub.load(\"./openimages_v4_ssd_mobilenet_v2_1\").signatures[\"default\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "0123bee6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    }
   ],
   "source": [
    "model3 = hub.load(\"./faster_rcnn_openimages_v4_inception_resnet_v2_1\").signatures[\"default\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "5a6f53e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "a99ca9e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "colorcodes = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "40418f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def drawbox(image,ymin,xmin,ymax,xmax,namewithscore,color):\n",
    "    im_height, im_width, _  = image.shape\n",
    "    left,top,right,bottom = int(xmin*im_width), int(ymin*im_height), int(xmax*im_width),int(ymax*im_height)\n",
    "    cv2.rectangle(image,(left,top),(right,bottom),color = color,thickness = 4)\n",
    "    FONT_SCALE = 5e-3\n",
    "    THICKNESS_SCALE = 4e-3\n",
    "    width = right-left\n",
    "    height = bottom-top\n",
    "    TEXT_Y_OFFSET_SCALE = 1e-2\n",
    "    cv2.rectangle(\n",
    "        image,\n",
    "        (left,top- int(height * 6e-2)),\n",
    "        (right,top),\n",
    "        color = color,\n",
    "        thickness = -1\n",
    "        \n",
    "    )\n",
    "    cv2.putText(\n",
    "        image,\n",
    "        namewithscore,\n",
    "        (left,top-int(height * TEXT_Y_OFFSET_SCALE)),\n",
    "        fontFace = cv2.FONT_HERSHEY_PLAIN,\n",
    "        fontScale = min(width,height)* FONT_SCALE,\n",
    "        thickness = math.ceil(min(width,height)* THICKNESS_SCALE),\n",
    "        color = (255,255,255)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "f7b7f4af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw(image,boxes,classnames,scores):\n",
    "    boxesidx = tf.image.non_max_suppression(boxes,scores,max_output_size = 4, iou_threshold = 0.5,score_threshold = 0.1)\n",
    "#     for i in range(len(boxes)):\n",
    "    for i in boxesidx:\n",
    "        ymin,xmin,ymax,xmax = tuple(boxes[i])\n",
    "        classname = classnames[i].decode(\"ascii\")\n",
    "        if classname in colorcodes.keys():\n",
    "            color = colorcodes[classname]\n",
    "        else:\n",
    "            c1 = random.randrange(0,255,30)\n",
    "            c2 = random.randrange(0,255,25)\n",
    "            c3 = random.randrange(0,255,50)\n",
    "            colorcodes.update({classname: (c1,c2,c3)})\n",
    "            color = colorcodes[classname]\n",
    "        namewithscore = \"{}:{}\".format(classname,int(100*scores[i]))\n",
    "        drawbox(image,ymin,xmin,ymax,xmax,namewithscore,color)\n",
    "        \n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "71e1b009",
   "metadata": {},
   "outputs": [],
   "source": [
    "image = cv2.imread(\"image.jpg\")\n",
    "image = cv2.resize(image,(800,600))\n",
    "image2 = cv2.cvtColor(image,cv2.COLOR_BGR2RGB)\n",
    "converted_img = tf.image.convert_image_dtype(image2,tf.float32)[tf.newaxis, ...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "4a68f686",
   "metadata": {},
   "outputs": [],
   "source": [
    "detection = model2(converted_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "54f8229c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[b'Clothing' b'Clothing' b'Man' b'Clothing' b'Person' b'Person' b'Person'\n",
      " b'Person' b'Clothing' b'Clothing' b'Person' b'Footwear' b'Person'\n",
      " b'Clothing' b'Table' b'Table' b'Human face' b'Woman' b'Footwear'\n",
      " b'Footwear' b'Man' b'Office building' b'Human arm' b'Clothing'\n",
      " b'Footwear' b'Table' b'Footwear' b'Footwear' b'Table' b'Man'\n",
      " b'Human face' b'Footwear' b'Clothing' b'Human face' b'Laptop' b'Table'\n",
      " b'Footwear' b'Woman' b'Person' b'Clothing' b'Table' b'Footwear'\n",
      " b'Footwear' b'Table' b'Human arm' b'Clothing' b'Footwear' b'Human arm'\n",
      " b'Human arm' b'Chair' b'Footwear' b'Footwear' b'Human arm' b'Footwear'\n",
      " b'Clothing' b'Footwear' b'Table' b'Clothing' b'Table' b'Person'\n",
      " b'Footwear' b'Footwear' b'Footwear' b'Building' b'Woman' b'Human hair'\n",
      " b'Table' b'Human arm' b'Table' b'Clothing' b'Table' b'Woman'\n",
      " b'Human hair' b'Human arm' b'Chair' b'Table' b'Footwear' b'Clothing'\n",
      " b'Footwear' b'Footwear' b'Human arm' b'Human arm' b'Human face'\n",
      " b'Human arm' b'Table' b'Table' b'Clothing' b'Table' b'Footwear' b'Table'\n",
      " b'Clothing' b'Clothing' b'Human arm' b'Table' b'Human face' b'Man'\n",
      " b'Chair' b'Clothing' b'Footwear' b'Footwear'], shape=(100,), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "print(detection[\"detection_class_entities\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "7a1146e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = {key:value.numpy() for key,value in detection.items()}\n",
    "imagewithboxes = draw(image,result['detection_boxes'],result['detection_class_entities'],result[\"detection_scores\"])\n",
    "cv2.imshow(\"image\",imagewithboxes)\n",
    "cv2.waitKey(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "dad8f5b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "image2 = cv2.imread(\"image2.jpg\")\n",
    "image3 = cv2.cvtColor(image2,cv2.COLOR_BGR2RGB)\n",
    "converted_img = tf.image.convert_image_dtype(image3,tf.float32)[tf.newaxis, ...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "a3504482",
   "metadata": {},
   "outputs": [],
   "source": [
    "detection = model3(converted_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "638a1f14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = {key:value.numpy() for key,value in detection.items()}\n",
    "imagewithboxes = draw(image2,result['detection_boxes'],result['detection_class_entities'],result[\"detection_scores\"])\n",
    "cv2.imwrite(\"detected.jpg\",imagewithboxes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "b3e72a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "image3 = cv2.imread(\"image3.jpg\")\n",
    "image4 = cv2.cvtColor(image3,cv2.COLOR_BGR2RGB)\n",
    "converted_img = tf.image.convert_image_dtype(image4,tf.float32)[tf.newaxis, ...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "243e316b",
   "metadata": {},
   "outputs": [],
   "source": [
    "detection = model3(converted_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "ba6bb766",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "68"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = {key:value.numpy() for key,value in detection.items()}\n",
    "imagewithboxes = draw(image3,result['detection_boxes'],result['detection_class_entities'],result[\"detection_scores\"])\n",
    "cv2.imshow(\"image\",imagewithboxes)\n",
    "cv2.waitKey(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d402e65",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
